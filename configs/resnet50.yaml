# FL-DomainNet-FAP-LoRA Configuration
# ResNet-50 optimized hyperparameters for federated learning experiment

# Dataset
data:
  root: "/root/domainnet"  # Path to DomainNet dataset
  domains: ["clipart", "infograph", "painting", "quickdraw", "real", "sketch"]
  num_classes: 126  # Use 126-class subset
  offload_pool_enabled: true  # Enable DC training on offload pool (optional)

# Client partitioning
partition:
  num_clients_per_domain: 24
  alpha: 0.1  # Dirichlet concentration parameter
  unload_ratio: 0.2  # ρ: fraction of data offloaded to DC (20% of training data)
  val_ratio: 0.1  # Fraction of data for validation
  seed: 42
  offload_pool_enabled: true  # Enable DC training on offload pool (optional)

# Model architecture
model:
  backbone: "resnet50"  # Use ResNet-50 backbone (2048-d features)
  pretrained: true
  lora:
    rank: 16  # r: LoRA rank
    alpha: 32  # α: LoRA scaling factor (increased from 16 for ResNet-50)
    attach_layers: ["layer4"]  # Attach LoRA to ResNet-50's layer4 Bottleneck blocks (conv3)

# Training
training:
  total_rounds: 200
  local_steps: 5  # Local training steps per client
  batch_size: 48  # Reduced from 64 to accommodate ~30% higher memory usage
  clients_participation: 0.2  # Fraction of clients participating per round

  # Learning rates
  lr_theta: 0.0003  # Learning rate for global backbone (3e-4)
  lr_phi: 0.001    # Learning rate for domain-specific parameters (1e-3)
  weight_decay: 0.0001  # Weight decay (1e-4)
  cosine_lr: true  # Use cosine learning rate schedule

# FAP-Float(S) Selector
selector:
  K: 5  # Aggregation period (select every K rounds)
  w1: 1.0  # Weight for loss L_e
  w2: 0.5  # Weight for drift Δ_e
  w3: 0.3  # Weight for coverage H_e
  w4: 0.2  # Weight for stability S_e^stay
  tau: 1.0  # Softmax temperature
  stability_bonus: 0.2  # Bonus for selecting same aggregator

# Edge Manager
edge_manager:
  ema_beta: 0.9  # Beta for exponential moving average of loss
  proj_dim: 64   # Dimension for random projection (prototypes)

# Logging and checkpointing
logging:
  log_interval: 1  # Log metrics every N rounds
  checkpoint_interval: 20  # Save checkpoints every N rounds
  output_dir: "./outputs"
  exp_name: ""  # Optional experiment name/tag (appended to timestamp)
  
  # New experiment tracking features
  use_timestamp_dir: true  # Use timestamp-based directory names (YYYY-MM-DD-HH-MM)
  save_config: true  # Save both original and effective configurations
  save_experiment_info: true  # Save experiment metadata (git, environment, timing)
  timestamp_format: "%Y-%m-%d-%H-%M"  # Timestamp format for directory names

# System
system:
  device: "cuda"  # Device for training (cuda/cpu)
  seed: 42       # Random seed for reproducibility
  num_workers: 4  # DataLoader workers

# Notes:
# - ResNet-50 uses 2048-d features (vs 512-d for ResNet-18)
# - LoRA alpha increased to 32 (from 16) to match increased model capacity
# - Batch size reduced to 48 (from 64) to accommodate higher memory usage (~18-20GB VRAM)
# - Requires 16GB+ GPU memory (24GB recommended for comfortable operation)
# - For memory-constrained GPUs, reduce batch_size to 32 or use gradient accumulation
